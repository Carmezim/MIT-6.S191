{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import lab1_utils as utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TensorFlow\n",
    "\n",
    "## What is a Computation Graph?\n",
    "\n",
    "Everything in TensorFlow comes down to building a computation graph. What is a computation graph? Its just a series of math operations that occur in some order. Here is an example of a simple computation graph:\n",
    "\n",
    "<img src=\"files/computation-graph.png\">\n",
    "\n",
    "This graph takes 2 inputs, (a, b) and computes an output (e). Each node in the graph is an operation that takes some input, does some computation, and passes its output to another node.\n",
    "\n",
    "We could make this computation graph in TensorFlow in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "c = tf.add(a, b)\n",
    "d = tf.sub(b, 1)\n",
    "e = tf.mul(c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses tf.placeholder to handle inputs to the model. This is like making a reservation at a restaurant. The restaurant reserves a spot for 5 people, but you are free to fill those seats with any set of friends you want to. tf.placeholder lets you specify that some input will be coming in, of some shape and some type. Only when you run the computation graph do you actually provide the values of this input data. You would run this simple computation graph like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    a_data, b_data = 3.0, 6.0\n",
    "    feed_dict = {a: a_data, b: b_data}\n",
    "    output = session.run([e], feed_dict=feed_dict)\n",
    "    print(output) # 45.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use feed_dict to pass in the actual input data into the graph. We use session.run to get the output from the c operation in the graph. Since e is at the end of the graph, this ends up running the entire graph and returning the number 45 - cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Networks in Tensorflow\n",
    "\n",
    "We can define neural networks in TensorFlow using computation graphs. Here is an example, very simple neural network (just 1 perceptron):\n",
    "\n",
    "<img src=\"files/computation-graph-2.png\">\n",
    "\n",
    "This graph takes an input, (x) and computes an output (out). It does it with what we learned in class, `out = sigmoid(W*x+b)`.\n",
    "\n",
    "We could make this computation graph in TensorFlow in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input_nodes = 2\n",
    "n_output_nodes = 1\n",
    "x = tf.placeholder(tf.float32, (None, n_input_nodes))\n",
    "W = tf.Variable(tf.ones((n_input_nodes, n_output_nodes)), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros(n_output_nodes), dtype=tf.float32)\n",
    "\n",
    "'''TODO: Define the operation for z (use tf.matmul to multiply W and x).'''\n",
    "z = tf.matmul(x, W) + b\n",
    "\n",
    "'''TODO: Define the operation for out (use tf.sigmoid).'''\n",
    "out = tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this graph, we again use session.run() and feed in our input via feed_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7310586]]\n"
     ]
    }
   ],
   "source": [
    "test_input = [[0.5, 0.5]]\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run(session=session)\n",
    "    feed_dict = {x: test_input}\n",
    "    output = session.run([out], feed_dict=feed_dict)\n",
    "    print(output[0]) # This should output 0.73105. If not, double-check your code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the value of a tf.Variable when we make it. Below is an example where we set the value of tf.Variable ourselves. We've made a classification dataset for you to play around with, and see how the decision boundary changes with the model parameters (weights and bias). Try to get all the datapoints correct (green)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLkkNGXdJasi4S1JDxl2SGvo/fR4EFehP/m8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26d533e84e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26d394a0208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions [array([[ 0.58661759]], dtype=float32), array([[ 0.57932425]], dtype=float32), array([[ 0.41580945]], dtype=float32), array([[ 0.4750208]], dtype=float32), array([[ 0.48250711]], dtype=float32), array([[ 0.57199615]], dtype=float32), array([[ 0.55724788]], dtype=float32), array([[ 0.62245935]], dtype=float32), array([[ 0.56463629]], dtype=float32), array([[ 0.4975]], dtype=float32), array([[ 0.4378235]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "'''TODO: manually optimize weight_values and bias_value to classify points'''\n",
    "\n",
    "# Modify weight_values, bias_value in the above code to adjust the decision boundary\n",
    "# See if you can classify all the points correctly (all markers green)\n",
    "\n",
    "weight_values = np.array([[0.03], [0.12]]) # TODO change values and re-run\n",
    "bias_value = np.array([[-0.55]]) #TODO change values and re-run\n",
    "\n",
    "# A pretty good boundary is made with:\n",
    "# weight_values = np.array([[0.03], [0.12]])\n",
    "# bias_value = np.array([[-0.5]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 2), name='x')\n",
    "W = tf.Variable(weight_values, name='W', dtype=tf.float32)\n",
    "b = tf.Variable(bias_value, name='b', dtype=tf.float32)\n",
    "z = tf.matmul(x, W) + b\n",
    "out = tf.sigmoid(z)\n",
    "\n",
    "data = np.array([[2, 7], [1, 7], [3, 1], [3, 3], [4, 3], [4, 6], [6, 5], [7, 7], [7, 5], [2, 4], [2, 2]])\n",
    "y = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0])\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run(session=session)\n",
    "    utils.classify_and_plot(data, y, x, out, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move to a real-world task. We're going to be classifying tweets as positive, negative, or neutral. Check out the very negative tweet below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/tweet-model.jpg\" style=\"width: 500px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "### Building an MLP\n",
    "\n",
    "MLP or Multi-layer perceptron is a basic archetecture where where we multiply our representation with some matrix `W` and add some bias `b` and then apply some nonlineanity like `tanh` at each layer. Layers are fully connected to the next. As the network gets deeper, it's expressive power grows exponentially and so they can draw some pretty fancy decision boundaries. In this exercise, you'll build your own MLP, with 2 hidden layers (layers that aren't input or output).\n",
    "\n",
    "To make training more stable and efficient, we'll actually evaluate 128 tweets at a time, and take gradients with respect to the loss on the 128. We call this idea **training with mini-batches**.\n",
    "\n",
    "### Step 1: Representing Words\n",
    "\n",
    "In this model, we’ll be representing tweets as [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) (BOW) representations. BOW representations are vectors where each element index represents a different word and its value represents the number of times this word appears in our sentence. This means that each sentence will be represented by a vector that is vocab_size long. Our output labels will be represented as a vector of size n_classes (3). We get this data with some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "Label: [ 0.  0.  1.]\n",
      "Bag of Words Representation: [ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X, y, index_to_word, sentences = utils.load_sentiment_data_bow()\n",
    "X_train, y_train, X_test, y_test = utils.split_data(X, y)\n",
    "vocab_size = X.shape[1]\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "print(\"Tweet:\", sentences[5])\n",
    "print(\"Label:\", y[5])\n",
    "print(\"Bag of Words Representation:\", X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Making Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our data loaded as numpy arrays. But remember, TensorFlow graphs begin with generic placeholder inputs, not actual data. We feed the actual data in later once the full graph has been defined. We define our placeholders like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_placeholder = tf.placeholder(tf.float32, shape=(None, vocab_size), name='data_placeholder')\n",
    "\n",
    "'''TODO: Make the labels placeholder.''';\n",
    "labels_placeholder = tf.placeholder(tf.float32, shape=(None, n_classes), name='labels_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Do We Pass in None?\n",
    "\n",
    "A note about ‘None’ and fluid-sized dimensions:\n",
    "\n",
    "You may notice that the first dimension of shape of data_placeholder is ‘None’. data_placeholder should have shape (num_tweets, vocab_size). However, we don’t know how many tweets we are going to be passing in at a time, num_tweets is unknown. Its possible that we only want to pass in 1 tweet at a time, or 30, or 1,000. Thankfully, TensorFlow allows us to specify placeholders with fluid-sized dimensions. We can use None to specify some fluid dimension of our shape. When our data eventually gets passed in as a numpy array, TensorFlow can figure out what the value of the fluid-size dimension should be.\n",
    "\n",
    "### Step 3: Define Network Parameters\n",
    "Let’s now define and initialize our network parameters.\n",
    "\n",
    "We'll our model parameters using tf.Variable. When you create a tf.Variable you pass a Tensor as its initial value to the Variable() constructor. A Tensor is a term for any N-dimensional matrix. There are a ton of different initial Tensor value functions you can use ([full list](https://www.tensorflow.org/api_docs/python/constant_op/)). All these functions take a list argument that determines their shape. Here we use tf.truncated_normal for our weights, and tf.zeros for our biases. Its important that the shape of these parameters are compatible. We’ll be matrix-multiplying the weights, so the last dimension of the previous weight matrix must equal the first dimension of the next weight matrix. Notice this pattern in the following tensor initialization code. Lastly, notice the size of the tensor for our last weights. We are predicting a vector of size n_classes so our network needs to end with n_classes nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Network Parameters\n",
    "\n",
    "# Here, we can define how many units will be in each hidden layer.\n",
    "n_hidden_units_h0 = 512 \n",
    "n_hidden_units_h1 = 256\n",
    "\n",
    "# Weights going from input to first hidden layer.\n",
    "# The first value passed into tf.Variable is initialization of the variable. \n",
    "# We initialize our weights to be sampled from a truncated normal, as this does something \n",
    "# called symmetry breaking and keeps the neural network from getting stuck at the start.\n",
    "# Since the weight matrix multiplies the previous layer to get inputs to the next layer,\n",
    "# its size is prev_layer_size x next_layer_size.\n",
    "h0_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocab_size, n_hidden_units_h0]),\n",
    "    name='h0_weights')\n",
    "h0_biases = tf.Variable(tf.zeros([n_hidden_units_h0]),\n",
    "                     name='h0_biases')\n",
    "\n",
    "'''TODO: Set up variables for the weights going into the second hidden layer.  \n",
    "You can check out the tf.Variable API here: https://www.tensorflow.org/api_docs/python/state_ops/variables.\n",
    "''';\n",
    "h1_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_hidden_units_h0, n_hidden_units_h1]),\n",
    "    name='h1_weights')\n",
    "h1_biases = tf.Variable(tf.zeros([n_hidden_units_h1]),\n",
    "                        name='h1_biases')\n",
    "\n",
    "# Weights going into the output layer.\n",
    "out_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_hidden_units_h1, n_classes]),\n",
    "    name='out_weights')\n",
    "out_biases = tf.Variable(tf.zeros([n_classes]),\n",
    "                     name='out_biases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Computation Graph\n",
    "\n",
    "Now let’s define our computation graph.\n",
    "\n",
    "Our first operation in our graph is a tf.matmul of our data input and our first set of weights. tf.matmul performs a matrix multiplication of two tensors. This is why it is so important that the dimensions of data_placeholder and h0_weights align (dimension 1 of data_placeholder must equal dimension 0 of h0_weights). We then just add the h0_bias variable and perform a nonlinearity transformation, in this case we use tf.nn.relu (ReLU). We do this again for the next hidden layer, and the final output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Computation Graphs\n",
    "hidden0 = tf.nn.relu(tf.matmul(data_placeholder, h0_weights) + h0_biases)\n",
    "\n",
    "'''TODO: write the computation to get the output of the second hidden layer.''';\n",
    "hidden1 = tf.nn.relu(tf.matmul(hidden0, h1_weights) + h1_biases)\n",
    "\n",
    "logits = tf.matmul(hidden1, out_weights) + out_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define a Loss Functions\n",
    "\n",
    "We have defined our network, but we need a way to train it. Training a network comes down to optimizing our network to minimize a loss function, or a measure how good we're doing.  Then, we can take the gradient with respect to that performance and move in the right direction.\n",
    "\n",
    "Since we are doing classification (pos vs neg), a common loss function to use is [cross entropy](https://colah.github.io/posts/2015-09-Visual-Information/):\n",
    "\n",
    "$$L( \\Theta ) = - \\sum_i  y_i'\\log{y_i}  $$\n",
    "\n",
    "where $y$ is our predicted probability distribution and $y'$ is the true distribution. We can access this function in tensorflow with `tf.nn.softmax_cross_entropy_with_logits`. \n",
    "\n",
    "Note that this loss is 0 if the prediction is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''TODO: Define the loss.  Use tf.nn.softmax_cross_entropy_with_logits.'''\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_placeholder))\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Define the optimizer operation.  This is what will take the derivate of the loss \n",
    "# with respect to each of our parameters and try to minimize it.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute the accuracy\n",
    "prediction_is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_placeholder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Conceptual Note:\n",
    "Nearly everything we do in TensorFlow is an operation with inputs and outputs. Our loss variable is an operation, that takes the output of the last layer of the net, which takes the output of the 2nd-to-last layer of the net, and so on. Our loss can be traced back to the input as a single function. This is our full computation graph. We pass this to our optimizer which is able to compute the gradient for this computation graph and adjust all the weights in it to minimize the loss.\n",
    "\n",
    "### Step 6: Training our Net\n",
    "We have our network, our loss function, and our optimizer ready, now we just need to pass in the data to train it. We pass in the data in chunks called mini-batches.  We do backpropogation at the end of a batch based on the loss that results from all the examples in the batch.  Using batches is just like Stochastic Gradient Descent, except instead of updating parameters after each example, we update them based on the gradient computed after *several* examples.\n",
    "\n",
    "To measure how well we're doing, we can't just look at how well our model performs on it's training data. It could be just memorizing the training data and perform terribly on data it hasn't seen before. To really measure how it performs in the wild, we need to present it with unseen data, and we can tune our hyper-parameters (like learning rate, num layers etc.) over this first unseen set, which we call our development (or validation) set. However, given that we optimized our hyper-parameters to the development set, to get a true fair assesment of the model, we test it with respect to a held-out test set at the end, and generally report those numbers.\n",
    "\n",
    "For now, we'll just use a training set and a testing set.  We'll train with the training set and evaluate on the test set to see how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train loss at step 0 : 477.461\n",
      "Minibatch train accuracy: 32.812%\n",
      "Test loss: 787.425\n",
      "Test accuracy: 19.672%\n",
      "Minibatch train loss at step 100 : 230.52\n",
      "Minibatch train accuracy: 51.562%\n",
      "Test loss: 199.547\n",
      "Test accuracy: 52.220%\n",
      "Minibatch train loss at step 200 : 150.591\n",
      "Minibatch train accuracy: 60.938%\n",
      "Test loss: 171.061\n",
      "Test accuracy: 56.626%\n",
      "Minibatch train loss at step 300 : 163.129\n",
      "Minibatch train accuracy: 59.375%\n",
      "Test loss: 162.722\n",
      "Test accuracy: 54.747%\n",
      "Minibatch train loss at step 400 : 178.134\n",
      "Minibatch train accuracy: 48.438%\n",
      "Test loss: 150.817\n",
      "Test accuracy: 55.908%\n",
      "Minibatch train loss at step 500 : 162.604\n",
      "Minibatch train accuracy: 49.219%\n",
      "Test loss: 149.401\n",
      "Test accuracy: 54.440%\n",
      "Minibatch train loss at step 600 : 150.363\n",
      "Minibatch train accuracy: 50.781%\n",
      "Test loss: 134.043\n",
      "Test accuracy: 58.026%\n",
      "Minibatch train loss at step 700 : 196.178\n",
      "Minibatch train accuracy: 40.625%\n",
      "Test loss: 133.165\n",
      "Test accuracy: 56.216%\n",
      "Minibatch train loss at step 800 : 125.809\n",
      "Minibatch train accuracy: 64.844%\n",
      "Test loss: 116.223\n",
      "Test accuracy: 61.134%\n",
      "Minibatch train loss at step 900 : 112.676\n",
      "Minibatch train accuracy: 61.719%\n",
      "Test loss: 108.210\n",
      "Test accuracy: 63.866%\n",
      "Minibatch train loss at step 1000 : 155.75\n",
      "Minibatch train accuracy: 54.688%\n",
      "Test loss: 109.242\n",
      "Test accuracy: 60.587%\n",
      "Minibatch train loss at step 1100 : 112.582\n",
      "Minibatch train accuracy: 61.719%\n",
      "Test loss: 101.207\n",
      "Test accuracy: 63.525%\n",
      "Minibatch train loss at step 1200 : 91.4542\n",
      "Minibatch train accuracy: 60.938%\n",
      "Test loss: 98.655\n",
      "Test accuracy: 63.251%\n",
      "Minibatch train loss at step 1300 : 137.4\n",
      "Minibatch train accuracy: 54.688%\n",
      "Test loss: 98.414\n",
      "Test accuracy: 62.022%\n",
      "Minibatch train loss at step 1400 : 144.645\n",
      "Minibatch train accuracy: 52.344%\n",
      "Test loss: 97.683\n",
      "Test accuracy: 61.544%\n",
      "Minibatch train loss at step 1500 : 74.8607\n",
      "Minibatch train accuracy: 63.281%\n",
      "Test loss: 93.961\n",
      "Test accuracy: 62.500%\n",
      "Minibatch train loss at step 1600 : 81.1654\n",
      "Minibatch train accuracy: 60.156%\n",
      "Test loss: 91.449\n",
      "Test accuracy: 62.739%\n",
      "Minibatch train loss at step 1700 : 57.825\n",
      "Minibatch train accuracy: 67.969%\n",
      "Test loss: 92.804\n",
      "Test accuracy: 61.270%\n",
      "Minibatch train loss at step 1800 : 53.5446\n",
      "Minibatch train accuracy: 75.781%\n",
      "Test loss: 83.765\n",
      "Test accuracy: 65.096%\n",
      "Minibatch train loss at step 1900 : 75.2037\n",
      "Minibatch train accuracy: 62.500%\n",
      "Test loss: 81.827\n",
      "Test accuracy: 64.959%\n",
      "Minibatch train loss at step 2000 : 63.8021\n",
      "Minibatch train accuracy: 71.875%\n",
      "Test loss: 80.559\n",
      "Test accuracy: 64.549%\n",
      "Minibatch train loss at step 2100 : 84.505\n",
      "Minibatch train accuracy: 59.375%\n",
      "Test loss: 77.583\n",
      "Test accuracy: 66.359%\n",
      "Minibatch train loss at step 2200 : 94.9914\n",
      "Minibatch train accuracy: 60.156%\n",
      "Test loss: 78.595\n",
      "Test accuracy: 63.934%\n",
      "Minibatch train loss at step 2300 : 78.5046\n",
      "Minibatch train accuracy: 54.688%\n",
      "Test loss: 76.959\n",
      "Test accuracy: 64.242%\n",
      "Minibatch train loss at step 2400 : 84.6077\n",
      "Minibatch train accuracy: 58.594%\n",
      "Test loss: 78.837\n",
      "Test accuracy: 62.466%\n",
      "Minibatch train loss at step 2500 : 96.2535\n",
      "Minibatch train accuracy: 50.000%\n",
      "Test loss: 74.036\n",
      "Test accuracy: 64.857%\n",
      "Minibatch train loss at step 2600 : 114.624\n",
      "Minibatch train accuracy: 53.906%\n",
      "Test loss: 74.088\n",
      "Test accuracy: 63.866%\n",
      "Minibatch train loss at step 2700 : 54.2415\n",
      "Minibatch train accuracy: 75.781%\n",
      "Test loss: 71.891\n",
      "Test accuracy: 64.754%\n",
      "Minibatch train loss at step 2800 : 37.9714\n",
      "Minibatch train accuracy: 71.094%\n",
      "Test loss: 68.982\n",
      "Test accuracy: 66.906%\n",
      "Minibatch train loss at step 2900 : 68.1411\n",
      "Minibatch train accuracy: 62.500%\n",
      "Test loss: 70.514\n",
      "Test accuracy: 63.627%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # this operation initializes all the variables we made earlier.\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Generate a minibatch.\n",
    "        offset = (step * batch_size) % (X_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Create a dictionary to pass in the batch data.\n",
    "        feed_dict_train = {data_placeholder: batch_data, labels_placeholder : batch_labels}\n",
    "        \n",
    "        # Run the optimizer, the loss, the predictions.\n",
    "        # We can run multiple things at once and get their outputs.\n",
    "        _, loss_value_train, predictions_value_train, accuracy_value_train = session.run(\n",
    "          [optimizer, loss, prediction, accuracy], feed_dict=feed_dict_train)\n",
    "        \n",
    "        # Print stuff every once in a while.\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch train loss at step\", step, \":\", loss_value_train)\n",
    "            print(\"Minibatch train accuracy: %.3f%%\" % (accuracy_value_train*100))\n",
    "            feed_dict_test = {data_placeholder: X_test, labels_placeholder: y_test}\n",
    "            loss_value_test, predictions_value_test, accuracy_value_test = session.run(\n",
    "                [loss, prediction, accuracy], feed_dict=feed_dict_test)\n",
    "            print(\"Test loss: %.3f\" % loss_value_test)\n",
    "            print(\"Test accuracy: %.3f%%\" % (accuracy_value_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code, you’ll see the network train and output its performance as it learns. I was able to get it to 65.5% accuracy. This is just OK, considering random guessing gets you 33.3% accuracy. In the next tutorial, you'll learn some ways to improve upon this.\n",
    "\n",
    "## Concluding Thoughts\n",
    "This was a brief introduction into TensorFlow. There is so, so much more to learn and explore, but hopefully this has given you some base knowledge to expand upon. As an additional exercise, you can see what you can do with this code to improve the performance. Ideas include: randomizing mini-batches, making the network deeper, using word embeddings (see below) rather than bag-of-words, trying different optimizers (like Adam), different weight initializations. We’ll explore some of these tomorrow.\n",
    "\n",
    "#### More on Word Embeddings\n",
    "\n",
    "In this lab we used Bag-of-Words to represent a tweet.  Word Embeddings are a more meaningful representation.  The basic idea is we represent a word with a vector $\\phi$ by the context the word appears in. We do this by training a neural network to predict the context of words across a large training set. The weights of that neural network can then be thought of as a dense and useful representation that captures context. This is useful because now our representations of words captures actual semantic similarity.\n",
    "\n",
    "Word Embeddings capture all kinds of useful semantic relationships. For example, one cool emergent property is $ \\phi(king) - \\phi(queen) = \\phi(man) - \\phi(woman)$. To learn more about the magic behind word embeddings we recommend Chris Olah's [blog post](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/). A common tool for generating Word Embeddings is word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "c = tf.add(a, b)\n",
    "d = tf.sub(b, 1)\n",
    "e = tf.mul(c, d)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    a_data, b_data = 3.0, 6.0\n",
    "    feed_dict = {a: a_data, b: b_data}\n",
    "    output = session.run([e], feed_dict=feed_dict)\n",
    "    print(output) # 45.0\n",
    "    \n",
    "n_input_nodes = 2\n",
    "n_output_nodes = 1\n",
    "x = tf.placeholder(tf.float32, (None, n_input_nodes))\n",
    "W = tf.Variable(tf.ones((n_input_nodes, n_output_nodes)), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros(n_output_nodes), dtype=tf.float32)\n",
    "\n",
    "'''TODO: Define the operation for z (use tf.matmul to multiply W and x).'''\n",
    "z = tf.matmul(x, W) + b\n",
    "\n",
    "'''TODO: Define the operation for out (use tf.sigmoid).'''\n",
    "out = tf.sigmoid(z)\n",
    "\n",
    "\n",
    "test_input = [[0.5, 0.5]]\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run(session=session)\n",
    "    feed_dict = {x: test_input}\n",
    "    output = session.run([out], feed_dict=feed_dict)\n",
    "    print(output[0]) # This should output 0.73105. If not, double-check your code above\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "'''TODO: manually optimize weight_values and bias_value to classify points'''\n",
    "\n",
    "# Modify weight_values, bias_value in the above code to adjust the decision boundary\n",
    "# See if you can classify all the points correctly (all markers green)\n",
    "\n",
    "weight_values = np.array([[-0.1], [0.2]]) # TODO change values and re-run\n",
    "bias_value = np.array([[0.5]]) #TODO change values and re-run\n",
    "\n",
    "# A pretty good boundary is made with:\n",
    "weight_values = np.array([[0.03], [0.12]])\n",
    "bias_value = np.array([[-0.5]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 2), name='x')\n",
    "W = tf.Variable(weight_values, name='W', dtype=tf.float32)\n",
    "b = tf.Variable(bias_value, name='b', dtype=tf.float32)\n",
    "z = tf.matmul(x, W) + b\n",
    "out = tf.sigmoid(z)\n",
    "\n",
    "data = np.array([[2, 7], [1, 7], [3, 1], [3, 3], [4, 3], [4, 6], [6, 5], [7, 7], [7, 5], [2, 4], [2, 2]])\n",
    "y = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0])\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run(session=session)\n",
    "    utils.classify_and_plot(data, y, x, out, session)\n",
    "    \n",
    "    \n",
    "    \n",
    "# load data\n",
    "X, y, index_to_word, sentences = utils.load_sentiment_data_bow()\n",
    "X_train, y_train, X_test, y_test = utils.split_data(X, y)\n",
    "vocab_size = X.shape[1]\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "print(\"Tweet:\", sentences[5])\n",
    "print(\"Label:\", y[5])\n",
    "print(\"Bag of Words Representation:\", X_train[5])\n",
    "\n",
    "data_placeholder = tf.placeholder(tf.float32, shape=(None, vocab_size), name='data_placeholder')\n",
    "\n",
    "'''TODO: Make the labels placeholder.''';\n",
    "labels_placeholder = tf.placeholder(tf.float32, shape=(None, n_classes), name='labels_placeholder')\n",
    "\n",
    "\n",
    "# Define Network Parameters\n",
    "\n",
    "# Here, we can define how many units will be in each hidden layer.\n",
    "n_hidden_units_h0 = 512 \n",
    "n_hidden_units_h1 = 256\n",
    "\n",
    "# Weights going from input to first hidden layer.\n",
    "# The first value passed into tf.Variable is initialization of the variable. \n",
    "# We initialize our weights to be sampled from a truncated normal, as this does something \n",
    "# called symmetry breaking and keeps the neural network from getting stuck at the start.\n",
    "# Since the weight matrix multiplies the previous layer to get inputs to the next layer,\n",
    "# its size is prev_layer_size x next_layer_size.\n",
    "h0_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocab_size, n_hidden_units_h0]),\n",
    "    name='h0_weights')\n",
    "h0_biases = tf.Variable(tf.zeros([n_hidden_units_h0]),\n",
    "                     name='h0_biases')\n",
    "\n",
    "'''TODO: Set up variables for the weights going into the second hidden layer.  \n",
    "You can check out the tf.Variable API here: https://www.tensorflow.org/api_docs/python/state_ops/variables.\n",
    "''';\n",
    "h1_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_hidden_units_h0, n_hidden_units_h1]),\n",
    "    name='h1_weights')\n",
    "h1_biases = tf.Variable(tf.zeros([n_hidden_units_h1]),\n",
    "                     name='h1_biases')\n",
    "\n",
    "# Weights going into the output layer.\n",
    "out_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_hidden_units_h1, n_classes]),\n",
    "    name='out_weights')\n",
    "out_biases = tf.Variable(tf.zeros([n_classes]),\n",
    "                     name='out_biases')\n",
    "\n",
    "# Define Computation Graphs\n",
    "hidden0 = tf.nn.relu(tf.matmul(data_placeholder, h0_weights) + h0_biases)\n",
    "\n",
    "'''TODO: write the computation to get the output of the second hidden layer.''';\n",
    "hidden1 = tf.nn.relu(tf.matmul(hidden0, h1_weights) + h1_biases)\n",
    "\n",
    "logits = tf.matmul(hidden1, out_weights) + out_biases\n",
    "\n",
    "'''TODO: Define the loss.  Use tf.nn.softmax_cross_entropy_with_logits.'''\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, labels_placeholder))\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Define the optimizer operation.  This is what will take the derivate of the loss \n",
    "# with respect to each of our parameters and try to minimize it.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute the accuracy\n",
    "prediction_is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_placeholder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_is_correct, tf.float32))\n",
    "\n",
    "num_steps = 3000\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # this operation initializes all the variables we made earlier.\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Generate a minibatch.\n",
    "        offset = (step * batch_size) % (X_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Create a dictionary to pass in the batch data.\n",
    "        feed_dict_train = {data_placeholder: batch_data, labels_placeholder : batch_labels}\n",
    "        \n",
    "        # Run the optimizer, the loss, the predictions.\n",
    "        # We can run multiple things at once and get their outputs.\n",
    "        _, loss_value_train, predictions_value_train, accuracy_value_train = session.run(\n",
    "          [optimizer, loss, prediction, accuracy], feed_dict=feed_dict_train)\n",
    "        \n",
    "        # Print stuff every once in a while.\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch train loss at step\", step, \":\", loss_value_train)\n",
    "            print(\"Minibatch train accuracy: %.3f%%\" % (accuracy_value_train*100))\n",
    "            feed_dict_test = {data_placeholder: X_test, labels_placeholder: y_test}\n",
    "            loss_value_test, predictions_value_test, accuracy_value_test = session.run(\n",
    "                [loss, prediction, accuracy], feed_dict=feed_dict_test)\n",
    "            print(\"Test loss: %.3f\" % loss_value_test)\n",
    "            print(\"Test accuracy: %.3f%%\" % (accuracy_value_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
